{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SQuAD_bilstm_without_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qj0BLlwKHAi5",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xcv68eo3HA_S",
        "outputId": "bf5675b7-3395-4702-ad93-1d150913f597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import re\n",
        "import io\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import json\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Dropout, Dense, Activation\n",
        "from keras.layers import LSTM, Bidirectional, Input\n",
        "from keras.layers import concatenate,merge\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Dense, concatenate, Dropout, RepeatVector, Activation, merge, Lambda, Flatten, Reshape\n",
        "from keras.layers import LSTM, Bidirectional, TimeDistributed, GRU, AveragePooling1D, Reshape, GlobalAveragePooling1D\n",
        "from keras.layers import Recurrent, InputSpec\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import gcsfs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "84ReBe_nHJRa",
        "colab": {}
      },
      "source": [
        "# Data extraction from json files\n",
        "with open('train-v2.0.json') as data_file:\n",
        "    data = json.load(data_file)\n",
        "\n",
        "data_entity_list = []\n",
        "for item in data['data']:\n",
        "    for p in item['paragraphs']:\n",
        "        for qq in p['qas']:\n",
        "            for a in qq['answers']:\n",
        "                data_entity_list.append((p['context'],qq['question'],a['text'],a['answer_start']))\n",
        "\n",
        "with open('dev-v2.0.json') as data_file:\n",
        "    data1 = json.load(data_file)\n",
        "\n",
        "data_entity_list1 = []\n",
        "for item in data1['data']:\n",
        "    for p in item['paragraphs']:\n",
        "        for qq in p['qas']:\n",
        "            for a in qq['answers']:\n",
        "                data_entity_list1.append((p['context'],qq['question'],a['text'],a['answer_start']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FPoVjVCUJMr1",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.DataFrame(data_entity_list, columns=['context','question','answer','answer_start'])\n",
        "val=pd.DataFrame(data_entity_list1, columns=['context','question','answer','answer_start'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlogXVEupjmi",
        "colab_type": "code",
        "colab": {},
        "outputId": "dc37a72c-b2fb-4bf0-8834-3cfff13d0f26"
      },
      "source": [
        "import re\n",
        "df['answer_end']= 0\n",
        "for i in range(len(df)):\n",
        "    new=df['answer'][i].split()\n",
        "    df['answer_end'][i]=(len(new)-1)+df['answer_start'][i]\n",
        "\n",
        "val['answer_end']= 0\n",
        "for i in range(len(val)):\n",
        "    new=val['answer'][i].split()\n",
        "    val['answer_end'][i]=(len(new)-1)+val['answer_start'][i]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6onpS-SpjnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,y_train=df[['context','question']],df[['answer_start','answer_end']]\n",
        "X_test,y_test=val[['context','question']],val[['answer_start','answer_end']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L95W45svpjnm",
        "colab_type": "code",
        "colab": {},
        "outputId": "e70b4146-7683-4348-d82f-604b26879e99"
      },
      "source": [
        "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((86821, 2), (86821, 2), (20302, 2), (20302, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NbjgnXapjoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(X_train['context'])\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ2mnmHRpjo6",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d5d791c-4ef6-4e3b-9d2e-5aaf50465567"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86901"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG0dchSppjpb",
        "colab_type": "code",
        "colab": {},
        "outputId": "b5e4d9a8-23c3-420d-ec87-b0cd787fee5d"
      },
      "source": [
        "lenght_list=[]\n",
        "for l in X_train.question:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_ques= np.max(lenght_list)\n",
        "max_ques\n",
        "\n",
        "lenght_list=[]\n",
        "for l in X_train.context:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_doc= np.max(lenght_list)\n",
        "max_doc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "653"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX-O4Vf-pjp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = vocab_size\n",
        "EMBEDDING_DIM = 100\n",
        "MAX_SEQUENCE_LENGTH = max_doc\n",
        "\n",
        "f = open('glove.6B.100d.txt','r',encoding=\"utf8\")\n",
        "embedding_index = {}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_index[word] = coefs \n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((words, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAJwSfFlpjqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' \n",
        "To perform vectorization of text records, \n",
        "this function is taken from 'https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras' with necessary modifications.\n",
        "\n",
        "Vectorize the words to their respective index and pad context to max context length and question to max question length.\n",
        "Answers vectors are padded to the max context length as well.\n",
        "'''\n",
        "def vectorizeData(xContext, xQuestion, xAnswerBeing, xAnswerEnd, word_index, context_maxlen, question_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    YBegin = []\n",
        "    YEnd = []\n",
        "    for i in range(len(xContext)):\n",
        "        x = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a']  for w in xContext[i]]\n",
        "        xq = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a'] for w in xQuestion[i] ]\n",
        "        y_Begin =  np.zeros(len(xContext[i]))\n",
        "        y_Begin[xAnswerBeing[i]] = 1\n",
        "        y_End = np.zeros(len(xContext[i]))\n",
        "        y_End[xAnswerEnd[i]] = 1\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "        YBegin.append(y_Begin)\n",
        "        YEnd.append(y_End)\n",
        "    return pad_sequences(X, maxlen=context_maxlen, padding='post'), pad_sequences(Xq, maxlen=question_maxlen, padding='post'), pad_sequences(YBegin, maxlen=context_maxlen, padding='post'), pad_sequences(YEnd, maxlen=context_maxlen, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrvvK-9tpjqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,y_train=X_train.reset_index() ,y_train.reset_index()\n",
        "tX, tXq, tYBegin, tYEnd = vectorizeData(X_train['context'],X_train['question'],y_train['answer_start'],y_train['answer_end'],tokenizer.word_index,max_doc,max_ques)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqZZ__YUpjrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' \n",
        "To perform vectorization of text records, \n",
        "this function is taken from 'https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras' with necessary modifications.\n",
        "\n",
        "Vectorize the words to their respective index and pad context to max context length and question to max question length.\n",
        "Answers vectors are padded to the max context length as well.\n",
        "'''\n",
        "def vectorizeValData(xContext, xQuestion, word_index, context_maxlen, question_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    YBegin = []\n",
        "    YEnd = []\n",
        "    for i in range(len(xContext)):\n",
        "        x = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a']  for w in xContext[i]]\n",
        "        xq = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a'] for w in xQuestion[i] ]\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "\n",
        "    return pad_sequences(X, maxlen=context_maxlen, padding='post'), pad_sequences(Xq, maxlen=question_maxlen, padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK6dJRlOpjrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vX, vXq = vectorizeValData(X_test['context'],X_test['question'], tokenizer.word_index,max_doc,max_ques)\n",
        "\n",
        "vX, vXq, vStart, vEnd = vectorizeData(X_test['context'],X_test['question'],y_test['answer_start'],y_test['answer_end'],tokenizer.word_index,max_doc,max_ques)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "daqR8QDxKr_x",
        "colab": {}
      },
      "source": [
        "''' Question and Answer Encoder '''\n",
        "question_input = Input(shape=(question_maxlen,), dtype='int32', name='question_input')\n",
        "context_input = Input(shape=(context_maxlen,), dtype='int32', name='context_input')\n",
        "\n",
        "questionEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
        "                         weights=[embedding_matrix], \n",
        "                         input_length=question_maxlen, trainable=False)(question_input) #mask_zero=True, \n",
        "contextEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
        "                         weights=[embedding_matrix], \n",
        "                         input_length=context_maxlen, trainable=False)(context_input) #mask_zero=True, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5gAS4gYvNAK1",
        "colab": {}
      },
      "source": [
        "Q_h = Bidirectional(LSTM(100, return_sequences=True))(questionEmbd)\n",
        "C_h = Bidirectional(LSTM(100, return_sequences=True))(contextEmbd)\n",
        "\n",
        "merge1 = concatenate([Q_h, C_h], axis=1)\n",
        "QC_bilstm = Bidirectional(LSTM(100, return_sequences=True))(merge1)\n",
        "merge2 = concatenate([QC_bilstm, C_h], axis=1)\n",
        "\n",
        "start_bilstm =  Bidirectional(LSTM(100, return_sequences=True))(merge2)\n",
        "end_bilstm =  Bidirectional(LSTM(100, return_sequences=True))(merge2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pm0NFWJpNKvY",
        "colab": {}
      },
      "source": [
        "start_token_dense = LSTM(context_maxlen, activation='softmax')(start_bilstm)\n",
        "end_token_dense = LSTM(context_maxlen, activation='softmax')(end_bilstm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b9BMjQu7PG0s",
        "outputId": "0c463196-4a0b-4d06-a39c-5e9adb1dd2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "model = Model(input=[context_input, question_input], output=[start_token_dense, end_token_dense])\n",
        "adam = optimizers.Adam(lr=0.003)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question_input (InputLayer)     (None, 60)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "context_input (InputLayer)      (None, 300)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 60, 100)      10484900    question_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 300, 100)     10484900    context_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 60, 200)      160800      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 300, 200)     160800      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 360, 200)     0           bidirectional_1[0][0]            \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 360, 200)     240800      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 660, 200)     0           bidirectional_3[0][0]            \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 660, 200)     240800      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 660, 200)     240800      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 300)          601200      bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 300)          601200      bidirectional_5[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 23,216,200\n",
            "Trainable params: 2,246,400\n",
            "Non-trainable params: 20,969,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqTs0XdLpjto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tX = tX[:60000]\n",
        "tXq = tXq[:60000]\n",
        "tYBegin = tYBegin[:60000]\n",
        "tYEnd = tYEnd[:60000]\n",
        "vX = vX[:5000]\n",
        "vXq = vXq[:5000]\n",
        "vStart = vStart[:5000]\n",
        "vEnd = vEnd[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1DZE50EwPQ_H",
        "outputId": "e25896df-9c1d-4d90-d46e-260717157dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "checkpoint2 = ModelCheckpoint('squad_bilstm_without_atten_v1.h5', monitor='val_lstm_6_loss', verbose=1, save_best_only=True, mode='min', period=2)\n",
        "checkpoint3 = EarlyStopping(monitor='val_lstm_6_loss', min_delta=0.01, patience=7, verbose=1, mode='min')\n",
        "checkpoint4 = EarlyStopping(monitor='val_lstm_7_loss', min_delta=0.01, patience=7, verbose=1, mode='min')\n",
        "callbacks_list = [checkpoint2, checkpoint3, checkpoint4] #, checkpoint3, checkpoint4]\n",
        "EPOCHS = 12\n",
        "model.fit([tX, tXq], [tYBegin,tYEnd], epochs=EPOCHS, batch_size=128, shuffle=True, \n",
        "          validation_split=0.2, \\\n",
        "          callbacks=callbacks_list, use_multiprocessing=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/12\n",
            "48000/48000 [==============================] - 2418s 50ms/step - loss: 10.1516 - lstm_6_loss: 5.0621 - lstm_7_loss: 5.0895 - val_loss: 10.0846 - val_lstm_6_loss: 5.0231 - val_lstm_7_loss: 5.0623\n",
            "Epoch 2/12\n",
            "48000/48000 [==============================] - 2410s 50ms/step - loss: 10.0863 - lstm_6_loss: 5.0258 - lstm_7_loss: 5.0605 - val_loss: 10.0729 - val_lstm_6_loss: 5.0197 - val_lstm_7_loss: 5.0541\n",
            "\n",
            "Epoch 00002: val_lstm_6_loss improved from inf to 5.01966, saving model to squad_bilstm_without_atten_v1.h5\n",
            "Epoch 3/12\n",
            "48000/48000 [==============================] - 2386s 50ms/step - loss: 10.0918 - lstm_6_loss: 5.0276 - lstm_7_loss: 5.0641 - val_loss: 10.0707 - val_lstm_6_loss: 5.0166 - val_lstm_7_loss: 5.0549\n",
            "Epoch 4/12\n",
            "48000/48000 [==============================] - 2376s 49ms/step - loss: 10.0760 - lstm_6_loss: 5.0207 - lstm_7_loss: 5.0552 - val_loss: 10.0645 - val_lstm_6_loss: 5.0147 - val_lstm_7_loss: 5.0506\n",
            "\n",
            "Epoch 00004: val_lstm_6_loss improved from 5.01966 to 5.01468, saving model to squad_bilstm_without_atten_v1.h5\n",
            "Epoch 5/12\n",
            "48000/48000 [==============================] - 2394s 50ms/step - loss: 10.0741 - lstm_6_loss: 5.0198 - lstm_7_loss: 5.0543 - val_loss: 10.0676 - val_lstm_6_loss: 5.0193 - val_lstm_7_loss: 5.0491\n",
            "Epoch 6/12\n",
            "48000/48000 [==============================] - 2391s 50ms/step - loss: 10.0730 - lstm_6_loss: 5.0195 - lstm_7_loss: 5.0535 - val_loss: 10.0679 - val_lstm_6_loss: 5.0159 - val_lstm_7_loss: 5.0528\n",
            "\n",
            "Epoch 00006: val_lstm_6_loss did not improve from 5.01468\n",
            "Epoch 7/12\n",
            "48000/48000 [==============================] - 2402s 50ms/step - loss: 10.0716 - lstm_6_loss: 5.0187 - lstm_7_loss: 5.0529 - val_loss: 10.0637 - val_lstm_6_loss: 5.0136 - val_lstm_7_loss: 5.0510\n",
            "Epoch 8/12\n",
            "48000/48000 [==============================] - 2394s 50ms/step - loss: 10.0704 - lstm_6_loss: 5.0179 - lstm_7_loss: 5.0524 - val_loss: 10.0609 - val_lstm_6_loss: 5.0135 - val_lstm_7_loss: 5.0482\n",
            "\n",
            "Epoch 00008: val_lstm_6_loss improved from 5.01468 to 5.01350, saving model to squad_bilstm_without_atten_v1.h5\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1430c8def0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0C0SiaJh8eDG",
        "outputId": "b13e4a47-fe54-4ad5-c280-0900d7c9f738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "predictions = model.predict([vX, vXq], batch_size=128)\n",
        "print(predictions[0].shape, predictions[1].shape)\n",
        "# make class prediction\n",
        "ansBegin = np.zeros((predictions[0].shape[0],), dtype=np.int32)\n",
        "ansEnd = np.zeros((predictions[0].shape[0],),dtype=np.int32) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 300) (5000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gplo1WkrXbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(predictions[0].shape[0]):\n",
        "    ansBegin[i] = predictions[0][i, :].argmax()\n",
        "    ansEnd[i] = predictions[1][i, :].argmax()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1j2KRIsgyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15182dac-724c-44f4-befd-61980d2920d9"
      },
      "source": [
        "# F1-Score calculation\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "start_f1 = f1_score(vStart ,ansBegin, average=\"weighted\")\n",
        "end_f1 = f1_score(vEnd ,ansEnd, average=\"weighted\")\n",
        "\n",
        "#F1\n",
        "(start_f1 + end_f1) / 2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35731254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFPdwPWZxsdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f2b0963-d641-4e0a-8c53-e43c6893b240"
      },
      "source": [
        "# Exact Match calculation\n",
        "\n",
        "matched_token_count = 0\n",
        "\n",
        "for t in range(len(ansBegin)):\n",
        "  if ((ansBegin[t] == vStart[t]) and (ansEnd[t] == vEnd[t])):\n",
        "    matched_token_count += 1\n",
        "\n",
        "# EM\n",
        "matched_token_count / len(ansBegin)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gephQBBMxuOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wovz6UYi2Bnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "To calculate Bahdanau Attention, the given class is taken from \n",
        "https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras/blob/master/rnnlayer.py , as it was already implemented and available.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}