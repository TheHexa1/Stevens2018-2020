{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "NQ_BiLSTM_with_Bahdanau_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Suqul4zEe1",
        "colab_type": "code",
        "colab": {},
        "outputId": "01194300-2513-4c9b-bfe6-51b1cf31f6b2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Dropout, Dense, Activation\n",
        "from keras.layers import LSTM, Bidirectional, Input\n",
        "from keras.layers import concatenate,merge\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "finaldf=pd.read_csv('google_nq_final_processed.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNVI4eAmzEfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def removeTags(sent):\n",
        "    s = sent.strip().split()\n",
        "    \n",
        "    stop_words = ['<P>', '</P>', '<Table>', '</Table>', '<Tr>', '</Tr>', '<Ul>', '<Ol>', '<Dl>', '</Ul>', '</Ol>', \n",
        "             '</Dl>', '<Li>', '<Dd>', '<Dt>', '</Li>', '</Dd>', '</Dt>', '<H1>', '</H1>', '<H2>', '</H2>', '<H3>', '</H3>',\n",
        "            'wikipedia', '</Td>', '<Th>', '<H4>', '</H4>', '</Th>', '<Td>', 'Jump up', 'Jump to']\n",
        "    s = [w for w in s if w not in stop_words]\n",
        "    \n",
        "    s = \" \".join(s)\n",
        "    # replace unnecessary characters with space\n",
        "    s = re.sub(r\"[^A-Za-z0-9]\", \" \", str(s).lower())    \n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBJugxF1zEfk",
        "colab_type": "code",
        "colab": {},
        "outputId": "776cabc2-0cfa-4984-81ba-9531410cb90c"
      },
      "source": [
        "new_doc=[]\n",
        "new_ans=[]\n",
        "for i in range(len(finaldf)):\n",
        "    new_doc.append(removeTags(finaldf.iloc[i]['document_text']))\n",
        "    new_ans.append(removeTags(finaldf.iloc[i]['answer']))\n",
        "    \n",
        "finaldf['document']=new_doc\n",
        "finaldf['ans']=new_ans\n",
        "finaldf=finaldf.drop(columns=['document_text','answer'])\n",
        "len(finaldf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33347"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0xRIJgwzEfy",
        "colab_type": "code",
        "colab": {},
        "outputId": "33415bc6-2145-4d33-e0ac-b9ca100c5fe4"
      },
      "source": [
        "count_df=finaldf['document'].apply(lambda x: len(x))\n",
        "df_small = finaldf[count_df <= 3000]\n",
        "df_small.reset_index(inplace=True, drop=True)\n",
        "len(df_small)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOmyCOUczEf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=df_small[:1400]\n",
        "test=df_small[1400:]\n",
        "X_train,y_train=train[['document','question_text']],train[['start','end']]\n",
        "X_test,y_test=test[['document','question_text']],test[['start','end']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtKspLhGzEgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer() #normally its somethinglike 40800\n",
        "tokenizer.fit_on_texts(X_train['document'])\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQrEY2QSzEgX",
        "colab_type": "code",
        "colab": {},
        "outputId": "61b1394e-3387-4b91-853e-d4eac39ca9d1"
      },
      "source": [
        "lenght_list=[]\n",
        "for l in X_train.question_text:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_ques= np.max(lenght_list)\n",
        "max_ques\n",
        "\n",
        "lenght_list=[]\n",
        "for l in X_train.document:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_doc= np.max(lenght_list)\n",
        "max_doc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_9Bxj6azEgz",
        "colab_type": "code",
        "colab": {},
        "outputId": "e8c672ac-fa49-442a-84c5-88b9573ab78c"
      },
      "source": [
        "def loadGloveModel(gloveFile):\n",
        "    f = open(gloveFile,'r')\n",
        "    embedding_index = {}\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs \n",
        "    f.close()\n",
        "    print('Found %s word vectors.' % len(embedding_index))\n",
        "    return embedding_index\n",
        "\n",
        "print('Preparing embedding matrix.')\n",
        "embeddings_index = loadGloveModel('glove.6B.100d.txt')\n",
        "nb_words = vocab_size\n",
        "EMBEDDING_DIM = 100\n",
        "MAX_SEQUENCE_LENGTH = max_doc\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(nb_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "print('Embedding matrix completed.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors.\n",
            "Embedding matrix completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIe8yH0kzEhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' \n",
        "To perform vectorization of text records, \n",
        "these two functions are taken from 'https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras' with necessary modifications.\n",
        "\n",
        "Vectorize the words to their respective index and pad context to max context length and question to max question length.\n",
        "Answers vectors are padded to the max context length as well.\n",
        "'''\n",
        "def vectorizeData(xContext, xQuestion, xAnswerBeing, xAnswerEnd, word_index, context_maxlen, question_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    YBegin = []\n",
        "    YEnd = []\n",
        "    for i in range(len(xContext)):\n",
        "        x = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a']  for w in xContext[i]]\n",
        "        xq = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a'] for w in xQuestion[i] ]\n",
        "        y_Begin =  np.zeros(len(xContext[i]))\n",
        "        y_Begin[xAnswerBeing[i]] = 1\n",
        "        y_End = np.zeros(len(xContext[i]))\n",
        "        y_End[xAnswerEnd[i]] = 1\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "        YBegin.append(y_Begin)\n",
        "        YEnd.append(y_End)\n",
        "    return pad_sequences(X, maxlen=context_maxlen, padding='post'), pad_sequences(Xq, maxlen=question_maxlen, padding='post'), pad_sequences(YBegin, maxlen=context_maxlen, padding='post'), pad_sequences(YEnd, maxlen=context_maxlen, padding='post')\n",
        "\n",
        "def vectorizeValData(xContext, xQuestion, word_index, context_maxlen, question_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    YBegin = []\n",
        "    YEnd = []\n",
        "    for i in range(len(xContext)):\n",
        "        x = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a']  for w in xContext[i]]\n",
        "        xq = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a'] for w in xQuestion[i] ]\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "\n",
        "    return pad_sequences(X, maxlen=context_maxlen, padding='post'), pad_sequences(Xq, maxlen=question_maxlen, padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rsj0sUCzEhU",
        "colab_type": "code",
        "colab": {},
        "outputId": "2645b589-8378-4668-f714-541e97800e55"
      },
      "source": [
        "tX, tXq, tYBegin, tYEnd = vectorizeData(X_train['document'],X_train['question_text'],y_train['start'],y_train['end'],tokenizer.word_index,max_doc,max_ques)\n",
        "tX.shape, tXq.shape, tYBegin.shape, tYEnd.shape "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1400, 1066), (1400, 18), (1400, 1066), (1400, 1066))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFgqrklVzEhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Dense, Dropout, RepeatVector, Activation, Lambda, Flatten, Reshape\n",
        "from keras.layers import LSTM, Bidirectional, TimeDistributed, GRU, concatenate, Recurrent, InputSpec\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Attention\n",
        "import re\n",
        "import gcsfs\n",
        "from keras import initializers\n",
        "\n",
        "'''\n",
        "To calculate Bahdanau Attention, the given class is taken from \n",
        "https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras/blob/master/rnnlayer.py , as it was already implemented and available.\n",
        "'''\n",
        "# Bahdanau Attention \n",
        "class Attention(Recurrent):\n",
        "    def __init__(self, units, h_dim,\n",
        "                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
        "                 **kwargs):\n",
        "        self.units = units\n",
        "        self.h_dim = h_dim\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        self.input_dim = input_shape[2] - self.h_dim\n",
        "\n",
        "        if self.stateful:\n",
        "            self.reset_states()\n",
        "        else:\n",
        "            self.states = [None]\n",
        "\n",
        "        self.Wa = self.add_weight('{}_Wa'.format(self.name), (self.units, self.units),\n",
        "                                  initializer=self.recurrent_initializer)\n",
        "        self.Ua = self.add_weight('{}_Ua'.format(self.name), (self.h_dim, self.units),\n",
        "                                  initializer=self.recurrent_initializer)\n",
        "        self.Va = self.add_weight('{}_Va'.format(self.name),(self.units,1),\n",
        "                                  initializer=self.recurrent_initializer)\n",
        "        self.Wzr = self.add_weight('{}_Wzr'.format(self.name), (self.input_dim, 2 * self.units),\n",
        "                                 initializer=self.recurrent_initializer)\n",
        "        self.Uzr = self.add_weight('{}_Wzr'.format(self.name), (self.units, 2 * self.units),\n",
        "                                 initializer=self.recurrent_initializer)\n",
        "        self.Czr = self.add_weight('{}_Czr'.format(self.name), (self.h_dim, 2 * self.units),\n",
        "                                   initializer=self.recurrent_initializer)\n",
        "        self.W = self.add_weight('{}_W'.format(self.name), (self.input_dim, self.units),\n",
        "                                 initializer=self.recurrent_initializer)\n",
        "        self.U = self.add_weight('{}_U'.format(self.name), (self.units, self.units),\n",
        "                                 initializer=self.recurrent_initializer)\n",
        "        self.C = self.add_weight('{}_C'.format(self.name), (self.h_dim, self.units),\n",
        "                                 initializer=self.recurrent_initializer)\n",
        "        self.built = True\n",
        "\n",
        "    def reset_states(self):\n",
        "        assert self.stateful, 'Layer must be stateful.'\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        if not input_shape[0]:\n",
        "            raise ValueError('If a RNN is stateful, a complete '\n",
        "                             'input_shape must be provided '\n",
        "                             '(including batch size).')\n",
        "        if hasattr(self, 'states'):\n",
        "            K.set_value(self.states[0],\n",
        "                        np.zeros((input_shape[0], self.units)))\n",
        "        else:\n",
        "            self.states = [K.zeros((input_shape[0], self.units))]\n",
        "\n",
        "    def preprocess_input(self, inputs, training=None):\n",
        "        return inputs\n",
        "\n",
        "    def step(self, inputs, states):\n",
        "        h_tm1 = states[0]  # previous memory\n",
        "        h_tm1a = K.dot(h_tm1, self.Wa)\n",
        "        eij = K.dot(K.tanh(h_tm1a + K.dot(inputs[:, :self.h_dim], self.Ua)), self.Va)\n",
        "        eijs = K.repeat_elements(eij, self.h_dim, axis=1)\n",
        "\n",
        "        cisum = eijs*inputs[:, :self.h_dim]\n",
        "        \n",
        "        zr = K.sigmoid(K.dot(inputs[:, self.h_dim:], self.Wzr) + K.dot(h_tm1, self.Uzr) + K.dot(cisum, self.Czr))\n",
        "        zi = zr[:, :self.units]\n",
        "        ri = zr[:, self.units: 2 * self.units]\n",
        "        si_ = K.tanh(K.dot(inputs[:, self.h_dim:], self.W) + K.dot(ri*h_tm1, self.U) + K.dot(cisum, self.C))\n",
        "        si = (1-zi) * h_tm1 + zi * si_\n",
        "        return si, [si] #h_tm1, [h_tm1]\n",
        "\n",
        "    def get_constants(self, inputs, training=None):\n",
        "        constants = []\n",
        "        constants.append([K.cast_to_floatx(1.) for _ in range(3)])\n",
        "        return constants\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units,\n",
        "                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer)}\n",
        "        base_config = super(Attention, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSLYGTIUzEhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_input = Input(shape=(max_ques,), dtype='int32', name='question_input')\n",
        "context_input = Input(shape=(max_doc,), dtype='int32', name='context_input')\n",
        "questionEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
        "                         weights=[embedding_matrix], \n",
        "                         input_length=max_ques, trainable=False)(question_input)  \n",
        "contextEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
        "                         weights=[embedding_matrix], \n",
        "                         input_length=max_doc, trainable=False)(context_input) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZaF1JBozEiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q_h = Bidirectional(LSTM(100, return_sequences=True))(questionEmbd)\n",
        "C_h = Bidirectional(LSTM(100, return_sequences=True))(contextEmbd)\n",
        "\n",
        "merge1 = concatenate([Q_h, C_h], axis=1)\n",
        "atten_1 = Attention(100, 100, return_sequences=True)(merge1)\n",
        "dropout_1 = Dropout(0.25)(atten_1)\n",
        "\n",
        "QC_bilstm = Bidirectional(LSTM(100, return_sequences=True))(dropout_1)\n",
        "merge2 = concatenate([QC_bilstm, C_h], axis=1)\n",
        "dropout_2 = Dropout(0.5)(merge2)\n",
        "\n",
        "start_bilstm =  Bidirectional(LSTM(100, return_sequences=True))(dropout_2)\n",
        "end_bilstm =  Bidirectional(LSTM(100, return_sequences=True))(dropout_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhikh3IdzEia",
        "colab_type": "code",
        "colab": {},
        "outputId": "0c471dea-24a4-404d-ce19-aa9d43079d8f"
      },
      "source": [
        "start_token_dense = LSTM(max_doc, activation='softmax')(start_bilstm)\n",
        "\n",
        "end_token_dense = LSTM(max_doc, activation='softmax')(end_bilstm)\n",
        "\n",
        "model = Model(input=[context_input, question_input], output=[start_token_dense, end_token_dense])\n",
        "adam = optimizers.Adam(lr=0.003)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question_input (InputLayer)     (None, 18)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "context_input (InputLayer)      (None, 1066)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 18, 100)      3172000     question_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 1066, 100)    3172000     context_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 18, 200)      160800      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 1066, 200)    160800      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1084, 200)    0           bidirectional_3[0][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "attention_1 (Attention)         (None, 1084, 100)    110100      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1084, 100)    0           attention_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 1084, 200)    160800      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 2150, 200)    0           bidirectional_5[0][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 2150, 200)    0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 2150, 200)    240800      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 2150, 200)    240800      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   (None, 1066)         5402488     bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 1066)         5402488     bidirectional_7[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 18,223,076\n",
            "Trainable params: 11,879,076\n",
            "Non-trainable params: 6,344,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=[<tf.Tenso..., inputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOXRO4rXzEis",
        "colab_type": "code",
        "colab": {},
        "outputId": "66337797-9b7d-403f-dd73-28b35d482ec7"
      },
      "source": [
        "\n",
        "checkpoint2 = ModelCheckpoint('bilstm_with_atten_model.h5', monitor='val_dense_1_loss', verbose=1, save_best_only=True, mode='min', period=2)\n",
        "checkpoint3 = EarlyStopping(monitor='val_dense_1_loss', min_delta=0.01, patience=3, verbose=1, mode='auto')\n",
        "checkpoint4 = EarlyStopping(monitor='val_dense_2_loss', min_delta=0.01, patience=3, verbose=1, mode='auto')\n",
        "callbacks_list = [checkpoint2, checkpoint3, checkpoint4] \n",
        "EPOCHS = 10\n",
        "model.fit([tX, tXq], [tYBegin,tYEnd], epochs=EPOCHS, batch_size=64, shuffle=True, \n",
        "          validation_split=0.2, \\\n",
        "          callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 1120 samples, validate on 280 samples\n",
            "Epoch 1/10\n",
            "1120/1120 [==============================] - 125s 111ms/step - loss: 0.2801 - dense_1_loss: 0.1437 - dense_2_loss: 0.1537 - val_loss: 0.3390 - val_dense_1_loss: 0.1292 - val_dense_2_loss: 0.1674\n",
            "Epoch 2/10\n",
            "1120/1120 [==============================] - 121s 108ms/step - loss: 0.1680 - dense_1_loss: 0.0776 - dense_2_loss: 0.0857 - val_loss: 0.3445 - val_dense_1_loss: 0.1318 - val_dense_2_loss: 0.1696\n",
            "\n",
            "Epoch 00002: val_dense_1_loss improved from inf to 0.13179, saving model to bilstm_with_atten_model.h5\n",
            "Epoch 3/10\n",
            "1120/1120 [==============================] - 122s 109ms/step - loss: 0.1732 - dense_1_loss: 0.0806 - dense_2_loss: 0.0878 - val_loss: 0.3450 - val_dense_1_loss: 0.1311 - val_dense_2_loss: 0.1708\n",
            "Epoch 4/10\n",
            "1120/1120 [==============================] - 122s 109ms/step - loss: 0.1692 - dense_1_loss: 0.0785 - dense_2_loss: 0.0860 - val_loss: 0.3444 - val_dense_1_loss: 0.1311 - val_dense_2_loss: 0.1703\n",
            "\n",
            "Epoch 00004: val_dense_1_loss improved from 0.13179 to 0.13112, saving model to bilstm_with_atten_model.h5\n",
            "Epoch 00004: early stopping\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f0270441eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kPKHfI2zEi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install h5py\n",
        "import h5py\n",
        "\n",
        "model.save_weights('nq_with_attention_model_weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtRYc3bLzEjL",
        "colab_type": "code",
        "colab": {},
        "outputId": "f662c7f7-1161-4779-e374-a83afa2ae115"
      },
      "source": [
        "vX, vXq, vStart, vEnd = vectorizeData(X_test['document'],X_test['question_text'],y_test['start'],y_test['end'],tokenizer.word_index,max_doc,max_ques)\n",
        "\n",
        "predictions = model.predict([vX, vXq], batch_size=64)\n",
        "print(predictions[0].shape, predictions[1].shape)\n",
        "ansBegin = np.zeros((predictions[0].shape[0],), dtype=np.int32)\n",
        "ansEnd = np.zeros((predictions[0].shape[0],),dtype=np.int32) \n",
        "for i in range(predictions[0].shape[0]):\n",
        "    ansBegin[i] = predictions[0][i, :].argmax()\n",
        "    ansEnd[i] = predictions[1][i, :].argmax()\n",
        "print(ansBegin.min(), ansBegin.max(), ansEnd.min(), ansEnd.max())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(255, 1066) (255, 1066)\n",
            "19 336 33 1022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlkxNBkozEja",
        "colab_type": "code",
        "colab": {},
        "outputId": "35b2b63d-2d71-4f3b-9704-126bd3f7d178"
      },
      "source": [
        "# F1-Score calculation\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "start_f1 = f1_score(vStart ,ansBegin, average=\"weighted\")\n",
        "end_f1 = f1_score(vEnd ,ansEnd, average=\"weighted\")\n",
        "\n",
        "#F1\n",
        "(start_f1 + end_f1) / 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.087849312953528e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv87eFud0qbJ",
        "colab_type": "text"
      },
      "source": [
        "- Because of F1-score~0, which is wrong, we didn't calculate EM score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bszeW0X50v8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}