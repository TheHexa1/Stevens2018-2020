{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "NQ_BiLSTM_without_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLauloHwjyYY",
        "colab_type": "code",
        "colab": {},
        "outputId": "c596c400-2a65-4d89-a2c6-9581da9d7635"
      },
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Dropout, Dense, Activation\n",
        "from keras.layers import LSTM, Bidirectional, Input\n",
        "from keras.layers import concatenate,merge\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "finaldf=pd.read_csv('google_nq_final_processed.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL3fOIVNjyYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# Remove all tags\n",
        "def removeTags(sent):\n",
        "    s = sent.strip().split()\n",
        "    \n",
        "    stop_words = ['<P>', '</P>', '<Table>', '</Table>', '<Tr>', '</Tr>', '<Ul>', '<Ol>', '<Dl>', '</Ul>', '</Ol>', \\\n",
        "             '</Dl>', '<Li>', '<Dd>', '<Dt>', '</Li>', '</Dd>', '</Dt>', '<H1>', '</H1>', '<H2>', '</H2>', '<H3>', '</H3>',\n",
        "            'wikipedia', '</Td>', '<Th>', '<H4>', '</H4>', '</Th>', '<Td>', 'Jump up', 'Jump to']\n",
        "    s = [w for w in s if w not in stop_words]\n",
        "    \n",
        "    s = \" \".join(s)\n",
        "    # replace unnecessary characters with space\n",
        "    s = re.sub(r\"[^A-Za-z0-9]\", \" \", str(s).lower())    \n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiSpczsjyY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_doc=[]\n",
        "new_ans=[]\n",
        "for i in range(len(finaldf)):\n",
        "    new_doc.append(removeTags(finaldf.iloc[i]['document_text']))\n",
        "    new_ans.append(removeTags(finaldf.iloc[i]['answer']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE_2w5jrjyZH",
        "colab_type": "code",
        "colab": {},
        "outputId": "b1a66558-4494-4f30-af0d-a3d79a9d9030"
      },
      "source": [
        "finaldf['document']=new_doc\n",
        "finaldf['ans']=new_ans\n",
        "finaldf=finaldf.drop(columns=['document_text','answer'])\n",
        "len(finaldf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33347"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH31lAMdjyZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_df=finaldf['document'].apply(lambda x: len(x))\n",
        "df_small = finaldf[count_df <= 3000]\n",
        "df_small.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH-dxrp4jyZc",
        "colab_type": "code",
        "colab": {},
        "outputId": "b22e0591-6f88-42a5-9697-d80b16d96468"
      },
      "source": [
        "len(df_small)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAHbH4xYjyZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=df_small[:1400]\n",
        "test=df_small[1400:]\n",
        "X_train,y_train=train[['document','question_text']],train[['start','end']]\n",
        "X_test,y_test=test[['document','question_text']],test[['start','end']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUbxmmi0jyZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer() \n",
        "tokenizer.fit_on_texts(X_train['document'])\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jnNie5ijyZ4",
        "colab_type": "code",
        "colab": {},
        "outputId": "4a7e416d-3aa8-409a-8c2a-6a360a2452f7"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUx_fhtKjyaF",
        "colab_type": "code",
        "colab": {},
        "outputId": "15dfa58f-57da-4dd8-82dd-6968c017598e"
      },
      "source": [
        "lenght_list=[]\n",
        "for l in X_train.question_text:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_ques= np.max(lenght_list)\n",
        "max_ques\n",
        "\n",
        "lenght_list=[]\n",
        "for l in X_train.document:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_doc= np.max(lenght_list)\n",
        "max_doc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2q-TOKmjyaV",
        "colab_type": "code",
        "colab": {},
        "outputId": "74e361c6-0983-4135-b12f-c42dcb7fff19"
      },
      "source": [
        "# helper to load Glove embeddings\n",
        "def loadGloveModel(gloveFile):\n",
        "    f = open(gloveFile,'r')\n",
        "    embedding_index = {}\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs \n",
        "    f.close()\n",
        "    print('Found %s word vectors.' % len(embedding_index))\n",
        "    return embedding_index\n",
        "\n",
        "print('Preparing embedding matrix.')\n",
        "embeddings_index = loadGloveModel('glove.6B.100d.txt')\n",
        "\n",
        "nb_words = vocab_size\n",
        "EMBEDDING_DIM = 100\n",
        "MAX_SEQUENCE_LENGTH = max_doc\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(nb_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "print('Embedding matrix completed.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing embedding matrix.\n",
            "Found 400000 word vectors.\n",
            "Embedding matrix completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6dXKHZojyak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' \n",
        "To perform vectorization of text records, \n",
        "this function is taken from 'https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras' with necessary modifications.\n",
        "\n",
        "Vectorize the words to their respective index and pad context to max context length and question to max question length.\n",
        "Answers vectors are padded to the max context length as well.\n",
        "'''\n",
        "def vectorizeData(xContext, xQuestion, xAnswerBeing, xAnswerEnd, word_index, context_maxlen, question_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    YBegin = []\n",
        "    YEnd = []\n",
        "    for i in range(len(xContext)):\n",
        "        x = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a']  for w in xContext[i]]\n",
        "        xq = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a'] for w in xQuestion[i] ]\n",
        "        # map the first and last words of answer span to one-hot representations\n",
        "        y_Begin =  np.zeros(len(xContext[i]))\n",
        "        y_Begin[xAnswerBeing[i]] = 1\n",
        "        y_End = np.zeros(len(xContext[i]))\n",
        "        y_End[xAnswerEnd[i]] = 1\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "        YBegin.append(y_Begin)\n",
        "        YEnd.append(y_End)\n",
        "    return pad_sequences(X, maxlen=context_maxlen, padding='post'), pad_sequences(Xq, maxlen=question_maxlen, padding='post'), pad_sequences(YBegin, maxlen=context_maxlen, padding='post'), pad_sequences(YEnd, maxlen=context_maxlen, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrsBCHAUjyay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tX, tXq, tYBegin, tYEnd = vectorizeData(X_train['document'],X_train['question_text'],y_train['start'],y_train['end'],tokenizer.word_index,max_doc,max_ques)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsGJXHEgjybB",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d5d67ac-7214-4c76-a2df-7b9f6681ca27"
      },
      "source": [
        "tX.shape, tXq.shape, tYBegin.shape, tYEnd.shape "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1400, 1066), (1400, 18), (1400, 1066), (1400, 1066))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edhPIShsjybS",
        "colab_type": "code",
        "colab": {},
        "outputId": "e40fd820-0d9f-489c-ed11-805fb3e392ac"
      },
      "source": [
        "from keras import optimizers\n",
        "import keras\n",
        "\n",
        "question_input = Input(shape=(max_ques,), dtype='int32', name='question_input')\n",
        "context_input = Input(shape=(max_doc,), dtype='int32', name='context_input')\n",
        "\n",
        "questionEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
        "                         weights=[embedding_matrix], \n",
        "                         input_length=max_ques, trainable=False)(question_input) #mask_zero=True, \n",
        "contextEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,\n",
        "                         weights=[embedding_matrix], \n",
        "                         input_length=max_doc, trainable=False)(context_input) #mask_zero=True, \n",
        "Q_h = Bidirectional(LSTM(100, return_sequences=True))(questionEmbd)\n",
        "C_h = Bidirectional(LSTM(100, return_sequences=True))(contextEmbd)\n",
        "\n",
        "merge1 = concatenate([Q_h, C_h], axis=1)\n",
        "\n",
        "QC_bilstm = Bidirectional(LSTM(100, return_sequences=True))(merge1)\n",
        "merge2 = concatenate([QC_bilstm, C_h], axis=1)\n",
        "\n",
        "start_bilstm =  Bidirectional(LSTM(100, return_sequences=True))(merge2)\n",
        "end_bilstm =  Bidirectional(LSTM(100, return_sequences=True))(merge2)\n",
        "\n",
        "start_token_dense = LSTM(max_doc, activation='softmax')(start_bilstm)\n",
        "end_token_dense = LSTM(max_doc, activation='softmax')(end_bilstm)\n",
        "\n",
        "model = Model(input=[context_input, question_input], output=[start_token_dense, end_token_dense])\n",
        "adam = optimizers.Adam(lr=0.003)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question_input (InputLayer)     (None, 18)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "context_input (InputLayer)      (None, 1066)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 18, 100)      3172000     question_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1066, 100)    3172000     context_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 18, 200)      160800      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 1066, 200)    160800      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1084, 200)    0           bidirectional_1[0][0]            \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_3 (Bidirectional) (None, 1084, 200)    240800      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 2150, 200)    0           bidirectional_3[0][0]            \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 2150, 200)    240800      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 2150, 200)    240800      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 1066)         5402488     bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   (None, 1066)         5402488     bidirectional_5[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 18,192,976\n",
            "Trainable params: 11,848,976\n",
            "Non-trainable params: 6,344,000\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:72: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX-Hhxg1jybb",
        "colab_type": "code",
        "colab": {},
        "outputId": "9eb5b5e7-3c41-4209-bd81-c793364841c7"
      },
      "source": [
        "EPOCHS=100\n",
        "es1=keras.callbacks.EarlyStopping(monitor='val_dense_3_loss', min_delta=0.01, patience=7, verbose=1, mode=\"auto\")\n",
        "es2=keras.callbacks.EarlyStopping(monitor='val_dense_4_loss', min_delta=0.01, patience=7, verbose=1, mode=\"auto\")\n",
        "\n",
        "callbacks_list=[es1,es2]\n",
        "\n",
        "\n",
        "model.fit([tX,tXq],[tYBegin,tYEnd],epochs=EPOCHS,shuffle=False,validation_split=0.2,callbacks=callbacks_list,verbose=1,use_multiprocessing=True,\n",
        "          batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1120 samples, validate on 280 samples\n",
            "Epoch 1/100\n",
            "1120/1120 [==============================] - 214s 191ms/step - loss: 0.2910 - dense_3_loss: 0.1405 - dense_4_loss: 0.1505 - dense_3_accuracy: 0.0018 - dense_4_accuracy: 0.0000e+00 - val_loss: 0.3652 - val_dense_3_loss: 0.1573 - val_dense_4_loss: 0.1978 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1120/1120 [==============================] - 206s 184ms/step - loss: 0.2259 - dense_3_loss: 0.1004 - dense_4_loss: 0.1255 - dense_3_accuracy: 0.0089 - dense_4_accuracy: 0.0018 - val_loss: 0.3520 - val_dense_3_loss: 0.1462 - val_dense_4_loss: 0.1959 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "1120/1120 [==============================] - 208s 186ms/step - loss: 0.2113 - dense_3_loss: 0.0951 - dense_4_loss: 0.1162 - dense_3_accuracy: 0.0143 - dense_4_accuracy: 8.9286e-04 - val_loss: 0.3513 - val_dense_3_loss: 0.1459 - val_dense_4_loss: 0.1957 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 4/100\n",
            "1120/1120 [==============================] - 210s 187ms/step - loss: 0.2085 - dense_3_loss: 0.0908 - dense_4_loss: 0.1177 - dense_3_accuracy: 0.0188 - dense_4_accuracy: 0.0018 - val_loss: 0.3510 - val_dense_3_loss: 0.1452 - val_dense_4_loss: 0.1960 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 5/100\n",
            "1120/1120 [==============================] - 208s 186ms/step - loss: 0.1989 - dense_3_loss: 0.0853 - dense_4_loss: 0.1136 - dense_3_accuracy: 0.0348 - dense_4_accuracy: 0.0000e+00 - val_loss: 0.3506 - val_dense_3_loss: 0.1448 - val_dense_4_loss: 0.1960 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 6/100\n",
            "1120/1120 [==============================] - 208s 186ms/step - loss: 0.2035 - dense_3_loss: 0.0907 - dense_4_loss: 0.1128 - dense_3_accuracy: 0.0357 - dense_4_accuracy: 0.0000e+00 - val_loss: 0.3494 - val_dense_3_loss: 0.1446 - val_dense_4_loss: 0.1951 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 7/100\n",
            "1120/1120 [==============================] - 210s 187ms/step - loss: 0.1930 - dense_3_loss: 0.0862 - dense_4_loss: 0.1068 - dense_3_accuracy: 0.0321 - dense_4_accuracy: 0.0018 - val_loss: 0.3495 - val_dense_3_loss: 0.1442 - val_dense_4_loss: 0.1956 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 8/100\n",
            "1120/1120 [==============================] - 208s 186ms/step - loss: 0.1895 - dense_3_loss: 0.0915 - dense_4_loss: 0.0980 - dense_3_accuracy: 0.0348 - dense_4_accuracy: 8.9286e-04 - val_loss: 0.3506 - val_dense_3_loss: 0.1446 - val_dense_4_loss: 0.1962 - val_dense_3_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0000e+00\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fc4604c98d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKrLWF-ijybl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install h5py\n",
        "import h5py\n",
        "\n",
        "model.save_weights('my_model_weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--wlirixjybt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' \n",
        "To perform vectorization of text records, \n",
        "this function is taken from 'https://github.com/wentaozhu/recurrent-attention-for-QA-SQUAD-based-on-keras' with necessary modifications.\n",
        "\n",
        "Vectorize the words to their respective index and pad context to max context length and question to max question length.\n",
        "Answers vectors are padded to the max context length as well.\n",
        "'''\n",
        "def vectorizeValData(xContext, xQuestion, word_index, context_maxlen, question_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    YBegin = []\n",
        "    YEnd = []\n",
        "    for i in range(len(xContext)):\n",
        "        x = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a']  for w in xContext[i]]\n",
        "        xq = [word_index[w] if w in tokenizer.word_index else tokenizer.word_index['a'] for w in xQuestion[i] ]\n",
        "        \n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "\n",
        "    return pad_sequences(X, maxlen=context_maxlen, padding='post'), pad_sequences(Xq, maxlen=question_maxlen, padding='post')\n",
        "\n",
        "# vX,vXq=vectorizeValData(X_test['document'], X_test['question_text'], tokenizer.word_index, max_doc,max_ques)\n",
        "vX, vXq, vStart, vEnd = vectorizeData(X_test['document'],X_test['question_text'],y_test['start'],y_test['end'],tokenizer.word_index,max_doc,max_ques)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsgnVdxkjyb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "# trained_model = models.load_weights('nn_model.h5')\n",
        "\n",
        "predictions = model.predict([vX, vXq], batch_size=128)\n",
        "\n",
        "print(predictions[0].shape, predictions[1].shape)\n",
        "\n",
        "ansBegin = np.zeros((predictions[0].shape[0],), dtype=np.int32)\n",
        "ansEnd = np.zeros((predictions[0].shape[0],),dtype=np.int32) \n",
        "for i in range(predictions[0].shape[0]):\n",
        "    ansBegin[i] = predictions[0][i, :].argmax()\n",
        "    ansEnd[i] = predictions[1][i, :].argmax()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBPl5V8Xjycl",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ef2b371-4eb5-4a1f-f3b8-deb4aa6d6460"
      },
      "source": [
        "# F1-Score calculation\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "start_f1 = f1_score(vStart ,ansBegin, average=\"weighted\")\n",
        "end_f1 = f1_score(vEnd ,ansEnd, average=\"weighted\")\n",
        "\n",
        "#F1\n",
        "(start_f1 + end_f1) / 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0035677428866283355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npQsqzI1qZqC",
        "colab_type": "text"
      },
      "source": [
        "- Because F1 score=0, which is wrong, We didn't calculate EM score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWNk67uBqibe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}