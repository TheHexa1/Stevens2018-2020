{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.cluster import KMeansClusterer, cosine_distance\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper methods ###\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def prepare_df(train_file, test_file):       \n",
    "    \n",
    "    train_df = pd.DataFrame(load_json_file(train_file), columns=['text'])\n",
    "    \n",
    "    test_df = pd.DataFrame(load_json_file(test_file), columns=['text', 'labels'])\n",
    "    test_df['single_label'] = test_df['labels'].apply(lambda x: x[0])    \n",
    "    \n",
    "    return train_df, test_df    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmean(train_file, test_file):\n",
    "    \n",
    "    # dataset preparation\n",
    "    train_df, test_df = prepare_df(train_file, test_file)    \n",
    "\n",
    "    # parameters\n",
    "    n_clusters = 3\n",
    "    MIN_DF = 5\n",
    "    STOP_WORDS = 'english'\n",
    "    EPOCHS = 25\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=STOP_WORDS, min_df=MIN_DF) \n",
    "    tfidf= tfidf_vect.fit_transform(train_df['text'])\n",
    "\n",
    "    cluster_clf = KMeansClusterer(n_clusters, cosine_distance, repeats=EPOCHS)\n",
    "    clusters = cluster_clf.cluster(tfidf.toarray(), assign_clusters=True)\n",
    "\n",
    "    test_tfidf = tfidf_vect.transform(test_df['text'])\n",
    "\n",
    "    preds = [cluster_clf.classify(doc) for doc in test_tfidf.toarray()]\n",
    "\n",
    "    test_df['cluster_id'] = preds\n",
    "\n",
    "    # extract ground_truth labels for each cluter\n",
    "    cluster_0 = test_df['single_label'][test_df.cluster_id==0]\n",
    "    cluster_1 = test_df['single_label'][test_df.cluster_id==1]\n",
    "    cluster_2 = test_df['single_label'][test_df.cluster_id==2]\n",
    "\n",
    "    # cluster and ground_truth label mapping\n",
    "    cluster_dict = {0: list(nltk.FreqDist(cluster_0).keys())[0],\n",
    "                    1: list(nltk.FreqDist(cluster_1).keys())[0],\n",
    "                    2: list(nltk.FreqDist(cluster_2).keys())[0]}\n",
    "\n",
    "    # Map true label to cluster id\n",
    "    preds_label = [cluster_dict[i] for i in preds]\n",
    "\n",
    "    # confusion matrix/table\n",
    "    confusion_df = pd.DataFrame(list(zip(test_df[\"single_label\"].values, preds)),\\\n",
    "                                columns = [\"actual class\", \"cluster\"])    \n",
    "    print(pd.crosstab(index=confusion_df.cluster, columns=confusion_df['actual class']))\n",
    "\n",
    "    # cluster and topic assigned to it\n",
    "    print('Cluster 0: Topic',cluster_dict[0])\n",
    "    print('Cluster 1: Topic',cluster_dict[1])\n",
    "    print('Cluster 2: Topic',cluster_dict[2])\n",
    "\n",
    "    # evaluation metrics\n",
    "    print(metrics.classification_report(test_df[\"single_label\"], preds_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_lda(train_file, test_file):\n",
    "    topic_assig = None\n",
    "    labels = None\n",
    "    \n",
    "    # dataset preparation\n",
    "    train_df, test_df = prepare_df(train_file, test_file)\n",
    "    \n",
    "    labels = test_df['labels']\n",
    "\n",
    "    n_topics = 3\n",
    "    MIN_DF = 5\n",
    "    MAX_DF = 0.9\n",
    "    STOP_WORDS = 'english'\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(max_df=MAX_DF, min_df=MIN_DF, stop_words=STOP_WORDS)\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(train_df['text'])\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=25, verbose=1, evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(tf)\n",
    "\n",
    "    tf_test = tf_vectorizer.transform(test_df['text'])\n",
    "    topic_assig=lda.transform(tf_test)\n",
    "    \n",
    "    preds = np.argmax(topic_assig, axis=1)\n",
    "\n",
    "    test_df['cluster_id'] = preds\n",
    "\n",
    "    # extract ground_truth labels for each cluter\n",
    "    cluster_0 = test_df['single_label'][test_df.cluster_id==0]\n",
    "    cluster_1 = test_df['single_label'][test_df.cluster_id==1]\n",
    "    cluster_2 = test_df['single_label'][test_df.cluster_id==2]\n",
    "\n",
    "    # cluster and ground_truth label mapping\n",
    "    cluster_dict = {0: list(nltk.FreqDist(cluster_0).keys())[0],\n",
    "                    1: list(nltk.FreqDist(cluster_1).keys())[0],\n",
    "                    2: list(nltk.FreqDist(cluster_2).keys())[0]}\n",
    "\n",
    "    # Map true label to cluster id\n",
    "    preds_label = [cluster_dict[i] for i in preds]\n",
    "\n",
    "    # confusion matrix/table\n",
    "    confusion_df = pd.DataFrame(list(zip(test_df[\"single_label\"].values, preds)),\\\n",
    "                                columns = [\"actual class\", \"cluster\"])    \n",
    "    print(pd.crosstab(index=confusion_df.cluster, columns=confusion_df['actual class']))\n",
    "\n",
    "    # cluster and topic assigned to it\n",
    "    print('Cluster 0: Topic',cluster_dict[0])\n",
    "    print('Cluster 1: Topic',cluster_dict[1])\n",
    "    print('Cluster 2: Topic',cluster_dict[2])\n",
    "\n",
    "    # evaluation metrics\n",
    "    print(metrics.classification_report(test_df[\"single_label\"], preds_label))\n",
    "    \n",
    "    return topic_assig, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_cluster(topic_assign, labels):\n",
    "    final_thresh, f1 = None, None\n",
    "    \n",
    "    df = pd.DataFrame(data=labels, columns=['labels'])\n",
    "    df['combined_labels'] = df['labels'].apply(lambda x: \"_\".join(x))\n",
    "    df['single_label'] = df['labels'].apply(lambda x: x[0])\n",
    "\n",
    "    unique_labels = list(sorted(set(df['combined_labels'])))\n",
    "    f1_score_l1 = [] #Travel&Transportation\n",
    "    f1_score_l2 = [] #Disaster and Accident\n",
    "    f1_score_l3 = [] #News and Economy\n",
    "    f1_score_l4 = [] #Disaster and Accident_Travel & Transportation\n",
    "    f1_score_l5 = [] #News and Economy_Travel & Transportation\n",
    "    threshold = []\n",
    "    prediction_df = pd.DataFrame(data=df['combined_labels'])\n",
    "\n",
    "    for t in arange(0.05, 0.95, 0.05):\n",
    "        preds = []\n",
    "        preds_label = []\n",
    "        threshold.append(t)\n",
    "        for row in range(topic_assign.shape[0]):        \n",
    "            labels_ = \"\"        \n",
    "            for topic in range(3):\n",
    "                if topic_assign[row][topic] > t:\n",
    "                    labels_ += str(topic)            \n",
    "\n",
    "            if labels_ == \"\":\n",
    "                preds.append(df.at[row, 'topic_id'])\n",
    "            else:\n",
    "                preds.append(labels_)\n",
    "\n",
    "        df['topic_id'] = preds\n",
    "\n",
    "        topic_0 = []\n",
    "        topic_01 = []\n",
    "        topic_02 = []\n",
    "        topic_1 = []\n",
    "        topic_12 = []\n",
    "        topic_2 = []\n",
    "\n",
    "        for i, row in df['combined_labels'].items():\n",
    "    \n",
    "            if '0' in df['topic_id'][i]:    \n",
    "                topic_0.append((row))\n",
    "            if '1' in df['topic_id'][i]:\n",
    "                topic_1.append(row)\n",
    "            if '2' in df['topic_id'][i]:   \n",
    "                topic_2.append((row))\n",
    "\n",
    "        topic_dict = { \n",
    "                        '0': nltk.FreqDist(topic_0).most_common()[0][0],\n",
    "                        '01': nltk.FreqDist(topic_1).most_common()[0][0]+'_'+\n",
    "                                nltk.FreqDist(topic_0).most_common()[0][0],\n",
    "                        '02': nltk.FreqDist(topic_2).most_common()[0][0]+'_'+\n",
    "                                nltk.FreqDist(topic_0).most_common()[0][0],\n",
    "                        '1':  nltk.FreqDist(topic_1).most_common()[0][0],\n",
    "                        '12': nltk.FreqDist(topic_2).most_common()[0][0]+'_'+\n",
    "                                nltk.FreqDist(topic_1).most_common()[0][0],\n",
    "                        '2':  nltk.FreqDist(topic_2).most_common()[0][0],\n",
    "                        '012': nltk.FreqDist(topic_0).most_common()[0][0]+'_'+\n",
    "                                nltk.FreqDist(topic_1).most_common()[0][0]+'_'+\n",
    "                                nltk.FreqDist(topic_2).most_common()[0][0]}\n",
    "        \n",
    "        for i in preds:\n",
    "            preds_label.append(topic_dict[i])\n",
    "\n",
    "        prediction_df[t] = preds_label\n",
    "        scores = f1_score(df['combined_labels'], preds_label, unique_labels, average=None)\n",
    "        \n",
    "        f1_score_l2.append(scores[0]) #disaster\n",
    "        f1_score_l3.append(scores[2]) #news\n",
    "        f1_score_l1.append(scores[4]) #travel\n",
    "\n",
    "        combined_label_scores = f1_score(df['combined_labels'], preds_label, unique_labels, average=None)\n",
    "        f1_score_l4.append(combined_label_scores[4]) #disaster_and_travel\n",
    "        f1_score_l5.append(combined_label_scores[0]) #news_and_travel\n",
    "\n",
    "    print(unique_labels[0]+': ',threshold[np.argmax(f1_score_l2)])\n",
    "    print(unique_labels[2]+': ',threshold[np.argmax(f1_score_l3)])\n",
    "    print(unique_labels[4]+': ',threshold[np.argmax(f1_score_l1)])\n",
    "    print(\"\")\n",
    "    print(unique_labels[0]+': ',max(f1_score_l2))\n",
    "    print(unique_labels[2]+': ',max(f1_score_l3))\n",
    "    print(unique_labels[4]+': ',max(f1_score_l1))\n",
    "    \n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"################# Uncomment to display multilabel classification #######################\")\n",
    "#     print(unique_labels[4]+': ',max(f1_score_l4))\n",
    "#     print(unique_labels[0]+': ',max(f1_score_l5))\n",
    "#     print(\"\")\n",
    "#     print(unique_labels[4]+': ',threshold[np.argmax(f1_score_l4)])\n",
    "#     print(unique_labels[0]+': ',threshold[np.argmax(f1_score_l5)])\n",
    "\n",
    "#     prediction_df[prediction_df['combined_labels']=='News and Economy_Travel & Transportation']\n",
    "#     prediction_df[prediction_df['combined_labels']=='Disaster and Accident_Travel & Transportation']\n",
    "    \n",
    "    return final_thresh, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Q1 ###\n",
      "actual class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                66                 0                      126\n",
      "1                               135                13                       45\n",
      "2                                 9               193                       13\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.70      0.64      0.67       210\n",
      "       News and Economy       0.90      0.94      0.92       206\n",
      "Travel & Transportation       0.66      0.68      0.67       184\n",
      "\n",
      "              micro avg       0.76      0.76      0.76       600\n",
      "              macro avg       0.75      0.75      0.75       600\n",
      "           weighted avg       0.75      0.76      0.75       600\n",
      "\n",
      "\n",
      "### Q2 ###\n",
      "\n",
      "### Q3 ###\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Due to randomness, you won't get the exact result\n",
    "    # as shown here, but your result should be close\n",
    "    # if you tune the parameters carefully\n",
    "    print('### Q1 ###')\n",
    "    # Q1\n",
    "    cluster_kmean('train_text.json', 'test_text.json')\n",
    "    print(\"\\n### Q2 ###\")\n",
    "    # Q2\n",
    "#     topic_assign, labels = cluster_lda('train_text.json', 'test_text.json')\n",
    "    print(\"\\n### Q3 ###\")\n",
    "    # Q2\n",
    "#     threshold, f1 = overlapping_cluster(topic_assign, labels)\n",
    "#     print(threshold)\n",
    "#     print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                66                 4                      167\n",
      "1                               139                 7                        8\n",
      "2                                 5               195                        9\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.90      0.66      0.76       210\n",
      "       News and Economy       0.93      0.95      0.94       206\n",
      "Travel & Transportation       0.70      0.91      0.79       184\n",
      "\n",
      "              micro avg       0.83      0.83      0.83       600\n",
      "              macro avg       0.85      0.84      0.83       600\n",
      "           weighted avg       0.85      0.83      0.83       600\n",
      "\n",
      "actual class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                63                 3                      149\n",
      "1                               107                 4                       28\n",
      "2                                40               199                        7\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.77      0.51      0.61       210\n",
      "       News and Economy       0.81      0.97      0.88       206\n",
      "Travel & Transportation       0.69      0.81      0.75       184\n",
      "\n",
      "              micro avg       0.76      0.76      0.76       600\n",
      "              macro avg       0.76      0.76      0.75       600\n",
      "           weighted avg       0.76      0.76      0.75       600\n",
      "\n",
      "actual class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                63                 0                       82\n",
      "1                               137                21                       88\n",
      "2                                10               185                       14\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.56      0.65      0.60       210\n",
      "       News and Economy       0.89      0.90      0.89       206\n",
      "Travel & Transportation       0.57      0.45      0.50       184\n",
      "\n",
      "              micro avg       0.67      0.67      0.67       600\n",
      "              macro avg       0.67      0.67      0.66       600\n",
      "           weighted avg       0.67      0.67      0.67       600\n",
      "\n",
      "actual class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                35                 2                      137\n",
      "1                               134                 7                       38\n",
      "2                                41               197                        9\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Travel & Transportation\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.00      0.00      0.00       210\n",
      "       News and Economy       0.80      0.96      0.87       206\n",
      "Travel & Transportation       0.50      0.95      0.65       184\n",
      "\n",
      "              micro avg       0.62      0.62      0.62       600\n",
      "              macro avg       0.43      0.64      0.51       600\n",
      "           weighted avg       0.43      0.62      0.50       600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual class  Disaster and Accident  News and Economy  Travel & Transportation\n",
      "cluster                                                                       \n",
      "0                                67                 0                      130\n",
      "1                               134                14                       40\n",
      "2                                 9               192                       14\n",
      "Cluster 0: Topic Travel & Transportation\n",
      "Cluster 1: Topic Disaster and Accident\n",
      "Cluster 2: Topic News and Economy\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "  Disaster and Accident       0.71      0.64      0.67       210\n",
      "       News and Economy       0.89      0.93      0.91       206\n",
      "Travel & Transportation       0.66      0.71      0.68       184\n",
      "\n",
      "              micro avg       0.76      0.76      0.76       600\n",
      "              macro avg       0.76      0.76      0.76       600\n",
      "           weighted avg       0.76      0.76      0.76       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_kmean('train_text.json', 'test_text.json')\n",
    "cluster_kmean('train_text.json', 'test_text.json')\n",
    "cluster_kmean('train_text.json', 'test_text.json')\n",
    "cluster_kmean('train_text.json', 'test_text.json')\n",
    "cluster_kmean('train_text.json', 'test_text.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
