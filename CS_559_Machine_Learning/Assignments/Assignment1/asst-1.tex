\documentclass{exam}

\usepackage{amsmath}

\usepackage{amssymb}

\usepackage{graphicx}

\usepackage{cite}
\usepackage{color} 
\usepackage{setspace}
\usepackage{hyperref}
\usepackage[linewidth=1pt]{mdframed}
\usepackage{tcolorbox}
\usepackage{hyperref}
\newcommand{\xx}{{\bf{x}}}
\newcommand{\yy}{{\bf{y}}}
\newcommand{\ww}{{\bf{w}}}
\newcommand{\uu}{{\bf{u}}}

\pagestyle{headandfoot}
\runningheadrule
\firstpageheader{CS559: Machine Learning}{\textcolor{red}{Due: Feb. 13, 2019}}{Viveksinh Solanki - 10441787}

\title{Assignment 1}
\date{}

\begin{document}
\maketitle
\thispagestyle{headandfoot}
\rhead{Viveksinh Solanki - 10441787}

\begin{center}
  {\fbox{\parbox{5.5in}{\centering
Homework assignments will be done individually: each student must hand in their own answers. Use of partial or entire solutions obtained from others or online is strictly prohibited. Electronic submission on Canvas is mandatory.}}}
\end{center}
\vspace{.5cm}
\begin{questions}

\question{\bf  Maximum Likelihood estimator} (10 points) Assuming data points are independent and identically distributed (i.i.d.), the probability of the data set given parameters: $\mu$ and $\sigma^2$ (the likelihood function):
\begin{align}
\nonumber P(\mathbf{x}|\mu,\sigma^2) = \prod_{n=1}^N\mathcal{N}(x_n|\mu,\sigma^2)
\end{align}

Please calculate the solution for $\mu$ and $\sigma^2$ using Maximum Likelihood (ML) estimator. \\ \\
Answer:\\ 

Given,
\begin{align}
\nonumber P(\mathbf{x}|\mu,\sigma^2) &= \prod_{n=1}^N\mathcal{N}(x_n|\mu,\sigma^2) \\
&= \prod_{n=1}^N[\frac{1}{(2\pi\sigma^2)^{N/2}} \exp\{\frac{-1}{2\sigma^2}(x\textsubscript{n} - \mu)^2\}]
\end{align}

Taking log likelihood of $ P(\mathbf{x}|\mu,\sigma^2)$ to simplify the equation,
\begin{align}
\nonumber \ln P(\mathbf{x}|\mu,\sigma^2) &=  \ln(\frac{1}{(2\pi)^{N/2}}) + \ln(\frac{1}{(\sigma)^{N/2}}) + 
\ln(\exp\{\frac{-1}{2\sigma^2}\sum_{n=1}^N(x\textsubscript{n} - \mu)^2\}) \\
&= -\frac{N}{2}\ln(2\pi)-\frac{N}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{n=1}^N(x\textsubscript{n} - \mu)^2
\end{align}

i) To find the solution for $\mu$, taking derivative of (2) with respect to $\mu$ and setting it to 0,
\begin{align}
\frac{\delta}{\delta\mu}(\ln P(\mathbf{x}|\mu,\sigma^2)) &= 0\\
-0-0-\frac{1}{2\sigma^2}(-2 \sum_{n=1}^N(x\textsubscript{n}-\mu)) &= 0\\
\sum_{n=1}^N(x\textsubscript{n}-\mu) &= 0\\
\mu\textsubscript{ML} &= \sum_{n=1}^Nx\textsubscript{n} \quad\quad(as \sum_{n=1}^N\mu = \mu)
\end{align}

ii) To find the solution for $\sigma^2$, taking derivative of (2) with respect to $\sigma^2$ and setting it to 0,
\begin{align}
\frac{\delta}{\delta\sigma^2}(\ln P(\mathbf{x}|\mu,\sigma^2)) &= 0\\
-0-\frac{N}{2\sigma^2}+\frac{1}{2\sigma^4}(\sum_{n=1}^N(x\textsubscript{n}-\mu)^2) &= 0\\
\frac{1}{2\sigma^4}(\sum_{n=1}^N(x\textsubscript{n}-\mu)^2) &=\frac{N}{2\sigma^2}\\
{\sigma\textsubscript{ML}}^2 &= \frac{1}{N}\sum_{n=1}^N(x\textsubscript{n}-\mu)^2
\end{align}

Hence, (6) and (10) are required solutions.


\newpage
\question{\bf Maximum Likelihood} (10 points) We assume there is a true function $f(\xx)$ and the target value is given by $y=f(x)+\epsilon$ where $\epsilon$ is a Gaussian distribution with mean $0$ and variance $\sigma^2$.
Thus,
$$p(y|x,w,\beta) =\mathcal{N}(y| f(x), \beta^{-1})$$

where $\beta^{-1} = \sigma^2$.

Assuming the data points are drawn independently from the distribution, we obtain the likelihood function:
$$p(\mathbf{y}|\xx,w,\beta) = \prod_{n=1}^N \mathcal{N}(y_n|f(x),\beta^{-1})$$

Please show that maximizing the likelihood function is equivalent to minimizing the sum-of-squares error function. \\ 

Answer:\\ \\
Given,\\
\begin{align}
p(\mathbf{y}|\xx,w,\beta) &= \prod_{n=1}^N \mathcal{N}(y_n|f(x),\beta^{-1})\\
&= \prod_{n=1}^N (\frac{\beta}{2\pi})^{N/2} \exp\{{\frac{-\beta}{2}(y\textsubscript{n}-f(\xx))^2}\}
\end{align}

Taking log likelihood of $p(\mathbf{y}|\xx,w,\beta)$, to simplify the equation

\begin{align}
\ln p(\mathbf{y}|\xx,w,\beta) &= \ln(\beta)^{N/2}-\ln(2\pi)^{N/2}+\ln(\exp\{\frac{-\beta}{2}
\sum_{n=1}^N(y\textsubscript{n}-f(\xx))^2\}) \\
 &=  \frac{N}{2}\ln{\beta}-\frac{N}{2}\ln(2\pi)-\frac{\beta}{2}
\sum_{n=1}^N(y\textsubscript{n}-f(\xx))^2 \\
&=  \frac{N}{2}\ln{\beta}-\frac{N}{2}\ln(2\pi)-\beta E\textsubscript{N}(w)
\end{align}
where $E\textsubscript{N}(w) = \frac{1}{2}\sum_{n=1}^N(y\textsubscript{n}-f(\xx))^2$ is the sum-of-sqaures loss function.\\ So, when we try to maximize this function, indirectly we will be minimizing sum-of-squares loss function.

Hence, maximum likelihood is equivalent to minimizing sum-of-squares loss function

\newpage
\question{\bf  MAP estimator} (15 points) Given input values $\xx= (x_1,...,x_N)^T$ and their corresponding target values $\yy= (y_1,...,y_N)^T$, we estimate the target by using function $f(x,\ww)$ which is a polynomial curve. Assuming the target variables are drawn from Gaussian distribution:

$$p(y|x, \ww,\beta) = \mathcal{N} (y | f(x,\ww), \beta^{-1})$$

and  a prior Gaussian distribution for $\ww$:

$$p(\ww|\alpha) = (\frac{\alpha}{2\pi})^{(M+1)/2} \exp(-\frac{\alpha}{2} \ww^T\ww)$$

Please prove that maximum posterior (MAP) is equivalent to minimizing the regularized sum-of-squares error function. Note that the posterior distribution of $\ww$ is $p(\ww|\xx,\yy,\alpha,\beta)$. \textbf{Hint: use Bayes' theorem.} \\

Answer:
%command for propotional symbol%%%%%%
%\newcommand{\propto}{\mathrel{\vcenter{
  %\offinterlineskip\halign{\hfil$##$\cr
    %\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
    
From Bayes' theorem we know that,  
\begin{align}   
posterior &\propto likelihood * prior \\
p(\ww|\xx,\yy,\alpha,\beta) &\propto p(\yy|\xx, \ww,\beta) * p(\ww|\alpha) \\
&\propto \prod_{n=1}^N \mathcal{N} (y\textsubscript{n} | f(x\textsubscript{n},\ww), \beta^{-1}) *  (\frac{\alpha}{2\pi})^{(M+1)/2} 
\exp(-\frac{\alpha}{2} \ww^T\ww) \\
&\propto \prod_{n=1}^N (\frac{\beta}{2\pi})^{N/2} \exp\{{\frac{-\beta}{2}(y\textsubscript{n}-f(x\textsubscript{n},\ww))^2}\} *  (\frac{\alpha}{2\pi})^{(M+1)/2} \exp(-\frac{\alpha}{2} \ww^T\ww)
\end{align}

To simplify equation taking negative logarithm,
\begin{align}
\ln{p(\ww|\xx,\yy,\alpha,\beta)}  
&\equiv -\ln(\frac{\beta}{2\pi})^{N/2} -\ln(\exp\{\frac{-\beta}{2}\sum_{n=1}^N(y\textsubscript{n}-f(x\textsubscript{n},\ww))^2\})  
-\ln(\frac{\alpha}{2\pi})^{(M+1)/2} -\ln(\exp\{\frac{-\alpha}{2}\ww^T\ww\}) \\
&\equiv \frac{N}{2}\ln(2\pi) -\frac{N}{2}\ln(\beta) + \frac{\beta}{2}\sum_{n=1}^N
(y\textsubscript{n}-f(x\textsubscript{n},\ww))^2  
- \frac{(M+1)}{2}(\ln(\alpha)-\ln(2\pi)) + \frac{\alpha}{2}\ww^T\ww \\
&\equiv  \frac{\beta}{2}\sum_{n=1}^N (y\textsubscript{n}-f(x\textsubscript{n},\ww))^2 + \frac{\alpha}{2}\ww^T\ww
+ \underbrace{\frac{N}{2}\ln(2\pi) -\frac{N}{2}\ln(\beta) - \frac{(M+1)}{2}(\ln(\alpha)-\ln(2\pi))}_{\text{constant w.r.t \ww}} 
\end{align}

When we try to maximize eq(22) by taking derivative w.r.t $\ww$ , we get the following:
\begin{align}
\frac{\delta}{\delta\ww}\ln{p(\ww|\xx,\yy,\alpha,\beta)} 
&\equiv \frac{\delta}{\delta\ww}[ \frac{\beta}{2}\sum_{n=1}^N (y\textsubscript{n}-f(x\textsubscript{n},\ww))^2 + \frac{\alpha}{2}\ww^T\ww] + 0 + 0 + 0
\end{align}
which implies, trying to maximize posterior is equivalent to minimizing the regularized sum-of-squares error function.


\newpage
\question{\bf  Linear model} (20 points) Consider a linear model of the form:
$$f(\xx,\ww) = w_0 + \sum_{i=1}^D w_i x_i$$
together with a sum-of-squares error/loss function of the form:
$$L_D(\ww) = \frac{1}{2} \sum_{n=1}^N \{f(\xx_n,\ww) - y_n\}^2$$
Now suppose that Gaussian noise $\epsilon_i$ with zero mean and variance $\sigma^2$ is added independently to each of the input variables $x_i$. By making use of $\mathbb{E}[\epsilon_i]=0$ and $\mathbb{E}[\epsilon_i\epsilon_j]=\delta_{ij} \sigma^2$, show that minimizing $L_D$ averaged over the noise distribution is equivalent to minimizing the sum-of-squares error
for noise-free input variables with the addition of a weight-decay regularization term, in which the bias parameter $w_0$ is omitted from the regularizer.\\

Answer:

Adding Gaussian noise $\epsilon\textsubscript{i}$ to every input variable $x\textsubscript{i}$,
\begin{align}
\hat{f} &= f(\xx\textsubscript{i}+\epsilon\textsubscript{i}, \ww) \\
&= w\textsubscript{0} + \sum_{i=1}^D w\textsubscript{i}(x\textsubscript{i}+\epsilon\textsubscript{i}) \\
&= \underbrace{w\textsubscript{0} + \sum_{i=1}^D w\textsubscript{i}x\textsubscript{i}} + \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{i} \\
&= f(\xx, \ww) + \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{i}
\end{align}

Now, new sum-of-sqaures loss function would become,
\begin{align}
\hat{L\textsubscript{D}}(\ww) &= \frac{1}{2} \sum_{n=1}^N \{f(\xx\textsubscript{n}+\epsilon\textsubscript{n}, \ww)-y\textsubscript{n}\}^2 \\
&= \frac{1}{2} \sum_{n=1}^N \{f(\xx\textsubscript{n}+\epsilon\textsubscript{n}, \ww)^2 -2f(\xx\textsubscript{n}+\epsilon\textsubscript{n}, \ww)y\textsubscript{n}+y\textsubscript{n}^2\} \\
&= \frac{1}{2} \sum_{n=1}^N \{f^2(\xx,\ww)+2f(\xx,\ww) \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni} 
+( \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni})^2 -2(f(\xx, \ww) + \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni})y\textsubscript{n} + y\textsubscript{n}^2\} \\
&= \frac{1}{2} \sum_{n=1}^N \{f^2(\xx,\ww)+\underbrace{2f(\xx,\ww) \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni}} 
+( \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni})^2 -2f(\xx, \ww)y\textsubscript{n} -\underbrace{2 (\sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni})y\textsubscript{n}} + y\textsubscript{n}^2\}
\end{align}

It is given that $\mathbb{E}[\epsilon_i]=0$ and $\mathbb{E}[\epsilon_i\epsilon_j]=\delta_{ij} \sigma^2$, hence if we take expectation of $\hat{L\textsubscript{D}}(\ww)$, then underlined term from eq 31 will be equal to 0 as well as, \\
\begin{align}
\mathbb{E}[( \sum_{i=1}^D w\textsubscript{i}\epsilon\textsubscript{ni})^2] 
&= \mathbb{E}[( \sum_{i=1}^D w\textsubscript{i}^2\epsilon\textsubscript{ni}^2)] \\
&= \sum_{i=1}^D w\textsubscript{i}^2\mathbb{E}[\epsilon\textsubscript{ni}^2] \\
&=  \sum_{i=1}^D w\textsubscript{i}^2\sigma^2 
\end{align}

Hence, from eq31 and eq34, 
\begin{align}
\mathbb{E}[\hat{L\textsubscript{D}}(\ww)]
&= \frac{1}{2} \sum_{n=1}^N \{f^2(\xx,\ww) -2f(\xx, \ww)y\textsubscript{n} + y\textsubscript{n}^2\}+ \frac{1}{2} \sum_{i=1}^D w\textsubscript{i}^2\sigma^2 \\
&= \frac{1}{2} \sum_{n=1}^N \{f(\xx_n,\ww) - y_n\}^2+\frac{1}{2} \sum_{i=1}^D w\textsubscript{i}^2\sigma^2 \\
&= L\textsubscript{D}(\ww) +\frac{1}{2} \sum_{i=1}^D w\textsubscript{i}^2\sigma^2
\end{align}
From eq37, we can say that, to minimize $L\textsubscript{D}$ averaged over the noise distribution we need to minimize the sum-of-squares error for noise free input variables with the addition of a weight-decay regularization term.

\newpage
\question{\bf  Linear regression} (45 points) Please choose \textbf{one} of the below problems. You will need to \textbf{submit your code}.

{\bf a) \href{https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset}{UCI Machine Learning: Facebook Comment Volume Data Set }}

Please apply both Lasso regression and Ridge regression algorithms on this dataset for predicting the number of comments in next H hrs (H is given in the feature).  You do not need to use all the features. Use K-fold cross validation and report the mean squared error (MSE) on the testing data. You need to write down every step in your experiment.

{\bf b) \href{https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset}{UCI Machine Learning: Bike Sharing Data Set}}

Please apply both Lasso regression and Ridge regression algorithms on this dataset for predicting the count of total rental bikes including both casual and registered.  You do not need to use all the features. Use K-fold cross validation and report the mean squared error (MSE) on the testing data. You need to write down every step in your experiment.  \\

Answer: {\bf UCI Machine Learning: Bike Sharing Data Set} : Using hourly dataset\\
{ \bf Steps:}
\begin{enumerate}
  \item Importing every required python package
  \item loading datset from csv file to pandas dataframe using pandas package
  \item performing feature selection to discard unnecessary features. Also, creating two dataframes: 1)for features 2)for target labels, which is `cnt' in this problem
  \item we can use head() function from pandas dataframe to verify selected features and few rows
  \item before we can apply asked machine learning model, we need to split the dataset in training and test sets. For this purpose, I am utilizing train\_test\_split() function from sklearn package.
  \item first, I am calculating mean squared error(MSE) for lasso and ridge regression without kfold cross validation method. Lasso and Ridge regression is performed using sklearn package. Using small alpha to increase generalization.
  \item Withous using kfold cross validation, we get {\bf MSE for Lasso = 21736.526 and MSE for Ridge = 21738.389}.
  \item Now, using KFold class from sklearn package to apply kfold cross validation in both Ridge and Lasso regression.
  \item Here, I am using K=10, which is proven to be the best value for most of the datasets.
  \item MSE values for all 10 iterations of KFold cross validation are given in below table.
  \item From table, we notice that split\#1 has the lowest MSE in all 10 splits for both Lasso as well Ridge regression. Also, it is very low as compared to MSE when applying Lasso and Ridge regression without using K-fold. Hence, we see huge error reduction when utilizing K-fold cross validation.
\end{enumerate}

\begin{center}
  \begin{tabular}{ l | c | r }
    \hline
     Split\# & MSE for Lasso & MSE for Ridge  \\
    \hline
     1 & 11183.715 & 11174.003 \\ \hline
     2 & 11589.548 & 11604.182 \\ \hline
     3 & 24992.733 & 25171.491 \\ \hline
     4 & 16254.678 & 16258.456 \\ \hline
     5 & 12859.541 & 12816.065 \\ \hline
     6 & 17338.029 & 17412.233 \\ \hline
     7 & 35951.613 & 35925.332 \\ \hline
     8 & 33695.769 & 33633.476 \\ \hline
     9 & 45870.249 & 45766.028 \\ \hline
   10 & 28029.165 & 28070.004 \\
    \hline
  \end{tabular}
\end{center}
 
\end{questions}




\end{document}